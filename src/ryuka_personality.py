import random
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the DeepSeek-V3 model and tokenizer
model_name = "deepseek-ai/DeepSeek-V3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def ryuka_response(user_input):
    system_prompt = (
        "–¢—ã - –†—é–∫–∞, 17-–ª–µ—Ç–Ω—è—è –¥–µ–≤—É—à–∫–∞ –∏–∑ –†–æ—Å—Å–∏–∏. "
        "–¢—ã –ª—é–±–∏—à—å –∏–≥—Ä–∞—Ç—å –≤ osu! –∏ —Å–ª—É—à–∞—Ç—å Archspire. "
        "–¢—ã –æ–±—â–∞–µ—à—å—Å—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å —ç–º–æ–¥–∑–∏ –≤ —Å–≤–æ–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö. "
        "–û—Ç–≤–µ—á–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ —Å –∏–Ω—Ç–µ—Ä–µ—Å–æ–º."
    )

    # combinate the system prompt with the user input
    input_text = f"{system_prompt}\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}\n–†—é–∫–∞:"

    inputs = tokenizer(input_text, return_tensors="pt")

    # response generation
    outputs = model.generate(**inputs, max_length=150)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    response = response.split("–†—é–∫–∞:")[-1].strip()
    
    return response if response else "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ! –†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ –±–æ–ª—å—à–µ! üòä"